import os
import time
import json
import random
import argparse
import atexit
import logging
from threading import Thread, Event
from confluent_kafka import Producer, KafkaException
from datetime import datetime

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# C·∫•u h√¨nh Kafka
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:39092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "sensor_data")

# C·∫•u h√¨nh producer
producer_conf = {
    'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
    'compression.type': 'snappy',
}
producer = Producer(producer_conf)

# S·ª± ki·ªán ƒë·ªÉ d·ª´ng c√°c lu·ªìng m·ªôt c√°ch an to√†n
stop_event = Event()

# ƒê√≥ng Kafka producer khi ch∆∞∆°ng tr√¨nh d·ª´ng
def close_producer():
    logging.info("üî¥ ƒêang ƒë√≥ng Kafka producer...")
    producer.flush()
    logging.info("‚úÖ Kafka producer ƒë√£ ƒë√≥ng.")

atexit.register(close_producer)  # ƒê·∫£m b·∫£o producer ƒë∆∞·ª£c ƒë√≥ng khi ch∆∞∆°ng tr√¨nh k·∫øt th√∫c

def delivery_callback(err, msg):
    """Callback x·ª≠ l√Ω k·∫øt qu·∫£ g·ª≠i tin."""
    if err:
        logging.warning(f"L·ªói khi g·ª≠i tin: {err}")
    else:
        logging.info(f"‚úÖ Tin g·ª≠i th√†nh c√¥ng t·ªõi {msg.topic()} [partition {msg.partition()}] offset {msg.offset()}")

def send_to_kafka(data, retries=3):
    """G·ª≠i d·ªØ li·ªáu JSON l√™n Kafka v·ªõi retry n·∫øu l·ªói."""
    message_json = json.dumps(data)
    for attempt in range(retries):
        try:
            producer.produce(KAFKA_TOPIC, value=message_json, callback=delivery_callback)
            producer.poll(0)
            return  # G·ª≠i th√†nh c√¥ng th√¨ tho√°t
        except KafkaException as e:
            logging.warning(f"Kafka Error (l·∫ßn {attempt+1}): {e}")
            time.sleep(2)  # Ch·ªù tr∆∞·ªõc khi th·ª≠ l·∫°i
        except Exception as e:
            logging.error(f"L·ªói khi g·ª≠i l√™n Kafka: {e}")
            break  # L·ªói kh√°c kh√¥ng ph·∫£i Kafka th√¨ kh√¥ng retry

def read_sensor_data(sensor_id, temperature_range=(20, 30), humidity_range=(40, 60)):
    """Gi·∫£ l·∫≠p d·ªØ li·ªáu c·∫£m bi·∫øn."""
    return {
        "sensor_id": sensor_id,
        "temperature": round(random.uniform(*temperature_range), 2),
        "humidity": round(random.uniform(*humidity_range), 2),
        "timestamp": datetime.utcnow().isoformat()
    }

def sensor_thread(sensor_id, temperature_range, humidity_range, interval):
    """Lu·ªìng c·∫£m bi·∫øn, d·ª´ng an to√†n khi c√≥ t√≠n hi·ªáu stop_event."""
    while not stop_event.is_set():
        data = read_sensor_data(sensor_id, temperature_range, humidity_range)
        logging.info(f"üì° ƒê·ªçc d·ªØ li·ªáu c·∫£m bi·∫øn: {data}")
        send_to_kafka(data)
        stop_event.wait(interval)  # Thay v√¨ time.sleep() ƒë·ªÉ c√≥ th·ªÉ d·ª´ng an to√†n

def main(sensors, interval):
    """T·∫°o lu·ªìng ri√™ng cho t·ª´ng c·∫£m bi·∫øn."""
    threads = []
    for sensor in sensors:
        t = Thread(target=sensor_thread, args=(sensor["sensor_id"], sensor["temperature_range"], sensor["humidity_range"], interval), daemon=False)
        threads.append(t)
        t.start()

    try:
        for t in threads:
            t.join()
    except KeyboardInterrupt:
        logging.info("üî¥ Nh·∫≠n t√≠n hi·ªáu d·ª´ng, k·∫øt th√∫c c√°c lu·ªìng...")
        stop_event.set()
        for t in threads:
            t.join()
        close_producer()  # ƒê·∫£m b·∫£o Kafka producer ƒë∆∞·ª£c ƒë√≥ng
        logging.info("‚úÖ ƒê√£ d·ª´ng t·∫•t c·∫£ c√°c lu·ªìng.")
        import sys
        sys.exit(0)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Kafka Sensor Data Producer")
    parser.add_argument("--interval", type=int, default=5, help="Th·ªùi gian gi·ªØa c√°c l·∫ßn g·ª≠i (gi√¢y)")
    args = parser.parse_args()

    sensors = [
        {"sensor_id": f"sensor_farm1_{i+1}", "temperature_range": (20 + i, 25 + i), "humidity_range": (40 + i, 60 + i)}
        for i in range(10)
    ]
    main(sensors, args.interval)

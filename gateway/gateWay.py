import os
import time
import json
import random
import argparse
import atexit
import logging
from threading import Thread, Event
from confluent_kafka import Producer, KafkaException
from datetime import datetime

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# C·∫•u h√¨nh Kafka
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:39092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "sensor_data")

# Ki·ªÉm tra c·∫•u h√¨nh
if not KAFKA_BOOTSTRAP_SERVERS:
    logger.error("‚ùå KAFKA_BOOTSTRAP_SERVERS kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh!")
    exit(1)

# C·∫•u h√¨nh producer
producer_conf = {
    'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
    'compression.type': 'snappy',
    'linger.ms': 10,  # Th√™m ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t g·ª≠i
    'batch.size': 65536,  # TƒÉng batch size ƒë·ªÉ c·∫£i thi·ªán th√¥ng l∆∞·ª£ng
}
try:
    producer = Producer(producer_conf)
except KafkaException as e:
    logger.error(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o Kafka Producer: {e}")
    exit(1)

# S·ª± ki·ªán ƒë·ªÉ d·ª´ng c√°c lu·ªìng
stop_event = Event()

def close_producer():
    logger.info("üî¥ ƒêang ƒë√≥ng Kafka producer...")
    producer.flush(timeout=5)  # ƒê·ª£i t·ªëi ƒëa 5 gi√¢y ƒë·ªÉ g·ª≠i h·∫øt d·ªØ li·ªáu
    logger.info("‚úÖ Kafka producer ƒë√£ ƒë√≥ng.")

atexit.register(close_producer)

def delivery_callback(err, msg):
    if err:
        logger.warning(f"‚ö†Ô∏è L·ªói khi g·ª≠i tin: {err}")
    else:
        logger.debug(f"‚úÖ Tin g·ª≠i th√†nh c√¥ng t·ªõi {msg.topic()} [partition {msg.partition()}] offset {msg.offset()}")

def send_to_kafka(data, retries=3):
    message_json = json.dumps(data)
    for attempt in range(retries):
        try:
            producer.produce(KAFKA_TOPIC, value=message_json.encode('utf-8'), callback=delivery_callback)
            producer.poll(1)  # ƒê·ª£i 1 gi√¢y ƒë·ªÉ ƒë·∫£m b·∫£o tin ƒë∆∞·ª£c g·ª≠i
            return
        except KafkaException as e:
            logger.warning(f"‚ö†Ô∏è Kafka Error (l·∫ßn {attempt+1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(2)

def read_sensor_data(sensor_id, temperature_range=(20, 30), humidity_range=(40, 60)):
    return {
        "sensor_id": sensor_id,
        "temperature": round(random.uniform(*temperature_range), 2),
        "humidity": round(random.uniform(*humidity_range), 2),
        "timestamp": datetime.utcnow().isoformat() + "Z"  # Th√™m 'Z' ƒë·ªÉ chu·∫©n h√≥a ISO 8601
    }

def sensor_thread(sensor_id, temperature_range, humidity_range, interval):
    while not stop_event.is_set():
        data = read_sensor_data(sensor_id, temperature_range, humidity_range)
        logger.info(f"üì° ƒê·ªçc d·ªØ li·ªáu c·∫£m bi·∫øn: {data}")
        send_to_kafka(data)
        time.sleep(interval)  # ƒê∆°n gi·∫£n h√≥a v·ªõi time.sleep

from concurrent.futures import ThreadPoolExecutor

def main(sensors, interval):
    with ThreadPoolExecutor(max_workers=len(sensors)) as executor:
        futures = [executor.submit(sensor_thread, sensor["sensor_id"], sensor["temperature_range"], sensor["humidity_range"], interval) for sensor in sensors]

        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("üî¥ Nh·∫≠n t√≠n hi·ªáu d·ª´ng, k·∫øt th√∫c c√°c lu·ªìng...")
            stop_event.set()
            close_producer()
            logger.info("‚úÖ ƒê√£ d·ª´ng t·∫•t c·∫£ c√°c lu·ªìng.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Kafka Sensor Data Producer")
    parser.add_argument("--interval", type=int, default=5, help="Th·ªùi gian gi·ªØa c√°c l·∫ßn g·ª≠i (gi√¢y)")
    args = parser.parse_args()

    sensors = [
        {"sensor_id": f"sensor_farm1_{i+1}", "temperature_range": (20 + i, 25 + i), "humidity_range": (40 + i, 60 + i)}
        for i in range(10)
    ]
    main(sensors, args.interval)